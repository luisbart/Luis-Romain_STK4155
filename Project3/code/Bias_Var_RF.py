# -*- coding: utf-8 -*-
"""
Created on Sat Dec  3 21:40:51 2022

@author: luis.barreiro
"""
'''This program performs MLP regression on a synthetic dataset generated by the Franke Function
and performs bootstrap resampling technique
The program returns plot of Bias Variance trade off against max_depth andmin_sample_leaf (up to 10)
Author: L Barreiro'''
#%%
#Set working directory
import os
os.chdir("C:/Users/luis.barreiro/Documents/GitHub/Projects_STK4155/Project3")
cwd = os.getcwd()
print(cwd)

#Import some libraries and modules
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from Functions import FrankeFunction
from sklearn.utils import resample


#%%
#Create Franke function dataset

np.random.seed(3)        #create same seed for random number every time

#For Ridge regression, set up the hyper-parameters to investigate
lmbd = 1e-6

#Number of bootstraps
n_bootstraps = 75

# Generate dataset with n observations
n = 100
x = np.random.uniform(0,1,n)
y = np.random.uniform(0,1,n)

#Define noise
var = 0.01
noise = np.random.normal(0,var,n)

z = FrankeFunction(x,y) + noise
z = z.reshape(z.shape[0],1)

x = np.array(x).reshape(n,1)
y = np.array(y).reshape(n,1)
x1 = np.hstack((x,y)).reshape(n,2)

#Split train (80%) and test(20%) data before looping on polynomial degree
x_train, x_test, z_train, z_test = train_test_split(x1, z, test_size=0.2)

   
#Scaling not needed
#%% Bias-variance tradeoff. Constrained decision tree by depth
#inizializing before looping:
max_depth=15
error = np.zeros(max_depth)
bias = np.zeros(max_depth)
variance = np.zeros(max_depth)

for n in range(max_depth):
    z_pred = np.empty((z_test.shape[0],n_bootstraps))
    for i in range(n_bootstraps):
        x_, z_ = resample(x_train,z_train)
        
        Random_Forest_model = RandomForestRegressor(max_depth=n+1)
        Random_Forest_model.fit(x_, z_)
        print("Test set accuracy with Random Forests: {:.2f}".format(Random_Forest_model.score(x_,z_)))
        z_pred[:, i] = Random_Forest_model.predict(x_test)
        
    error[n] = np.mean( np.mean((z_test - z_pred)**2, axis=1, keepdims=True) )
    bias[n] = np.mean( (z_test - np.mean(z_pred, axis=1, keepdims=True))**2 )
    variance[n] = np.mean( np.var(z_pred, axis=1, keepdims=True) )


plt.plot(range(1,max_depth+1), error[:], label = 'Error')
plt.plot(range(1,max_depth+1), bias[:], label = 'Bias')
plt.plot(range(1,max_depth+1), variance[:], label = 'Variance')
plt.ylabel('Error')
plt.xlabel('Model complexity: depth of the tree')
plt.title("Variance-Bias tradeoff for RF")
plt.xticks(np.arange(0, max_depth+1, step=2))  # Set label locations.
plt.legend()
plt.savefig("Results/bias_variance_tradeoff/RF_bias_var_tradeoff.png",dpi=150)
plt.show()

#Check that bias+variance=error
temp=error-(bias+variance)
print(temp)

#%% Bias-variance tradeoff. Constrained decision tree by leaf
#inizializing before looping:
min_samples_leaf=50
error = np.zeros(min_samples_leaf)
bias = np.zeros(min_samples_leaf)
variance = np.zeros(min_samples_leaf)

for n in range(min_samples_leaf):
    z_pred = np.empty((z_test.shape[0],n_bootstraps))
    for i in range(n_bootstraps):
        x_, z_ = resample(x_train,z_train)
        
        Random_Forest_model = RandomForestRegressor(min_samples_leaf=n+1)
        Random_Forest_model.fit(x_, z_)
        print("Test set accuracy with Random Forests: {:.2f}".format(Random_Forest_model.score(x_,z_)))
        z_pred[:, i] = Random_Forest_model.predict(x_test)
        
    error[n] = np.mean( np.mean((z_test - z_pred)**2, axis=1, keepdims=True) )
    bias[n] = np.mean( (z_test - np.mean(z_pred, axis=1, keepdims=True))**2 )
    variance[n] = np.mean( np.var(z_pred, axis=1, keepdims=True) )


plt.plot(range(1,min_samples_leaf+1), error[:], label = 'Error')
plt.plot(range(1,min_samples_leaf+1), bias[:], label = 'Bias')
plt.plot(range(1,min_samples_leaf+1), variance[:], label = 'Variance')
plt.ylabel('Error')
plt.xlabel('Model complexity: min samples leaf')
plt.title("Variance-Bias tradeoff for RF")
plt.xticks(np.arange(0, min_samples_leaf+1, step=5))  # Set label locations.
plt.legend()
plt.savefig("Results/bias_variance_tradeoff/RF_bias_var_tradeoff_v02.png",dpi=150)
plt.show()


#%%
#Check that bias+variance=error
temp=error-(bias+variance)
print(temp)
