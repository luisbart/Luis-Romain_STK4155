
#%%

# -*- coding: utf-8 -*-
"""
Created on Thu Dec  1 15:12:27 2022

@author: luis.barreiro
"""

'''This program performs OLS regression on a synthetic dataset generated by the Franke Function
and performs bootstrap resampling technique
The program returns plot of Bias Variance trade off against polynomial degree (up to 8 or 5) 
Author: L Barreiro'''
#%%
#Set working directory
import os
os.chdir("C:/Users/luis.barreiro/Documents/GitHub/Projects_STK4155/Project3")
cwd = os.getcwd()
print(cwd)

#Import some libraries and modules
import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.utils import resample
#import Functions module
from Functions import FrankeFunction, LinReg, DesignMatrix

#%%
#Create Franke function dataset

np.random.seed(3)  

#Bias-variance analysis of the Franke function

#Bootstrap
n_bootstraps = 75

# Make data set
n = 100

x = np.random.uniform(0,1,n)
y = np.random.uniform(0,1,n)
z = FrankeFunction(x, y)
z = z + np.random.normal(0,0.1,z.shape)

x = np.array(x).reshape(n,1)
y = np.array(y).reshape(n,1)
z = np.array(z).reshape(n,1)

x1 = np.hstack((x,y)).reshape(n,2)

#Split train (80%) and test(20%) data before looping on polynomial degree
x_train, x_test, z_train, z_test = train_test_split(x1, z, test_size=0.2)


#%% 
#Perform Bias-variance tradeoff with bootstrap
#OLS - up to degree 8
maxdegree = 8

#Initialize before looping:
error = np.zeros(maxdegree)
bias = np.zeros(maxdegree)
variance = np.zeros(maxdegree)
polydegree = np.zeros(maxdegree)

#Bootstrap LB
for degree in range(maxdegree):
    X_train = DesignMatrix(x_train[:,0],x_train[:,1],degree+1)
    X_test = DesignMatrix(x_test[:,0],x_test[:,1],degree+1)
    z_pred = np.zeros((z_test.shape[0], n_bootstraps))
    for i in range(n_bootstraps):
        x_, z_ = resample(X_train, z_train)
        z_fit, zpred, beta = LinReg(x_, X_test, z_)
        z_pred[:, i] = zpred.ravel()
  
    polydegree[degree] = degree+1
    error[degree] = np.mean( np.mean((z_test - z_pred)**2, axis=1, keepdims=True) )
    bias[degree] = np.mean( (z_test - np.mean(z_pred, axis=1, keepdims=True))**2 )
    variance[degree] = np.mean( np.var(z_pred, axis=1, keepdims=True) )


plt.plot(polydegree, error, label='Error')
plt.plot(polydegree, bias, label='bias')
plt.plot(polydegree, variance, label='Variance')
plt.xticks(np.arange(1, maxdegree+1, step=1))  # Set label locations.
plt.xlabel('Model complexity: polynomial degree')
plt.ylabel('Mean squared error')
plt.title("Variance-Bias tradeoff for OLS")
plt.legend()
plt.savefig("Results/bias_variance_tradeoff/OLS_bias_var_tradeoff.png",dpi=150)
plt.show()


#%% OLS - up to degree 5
maxdegree = 5

#Initialize before looping:
error = np.zeros(maxdegree)
bias = np.zeros(maxdegree)
variance = np.zeros(maxdegree)
polydegree = np.zeros(maxdegree)

#Bootstrap LB
for degree in range(maxdegree):
    X_train = DesignMatrix(x_train[:,0],x_train[:,1],degree+1)
    X_test = DesignMatrix(x_test[:,0],x_test[:,1],degree+1)
    z_pred = np.zeros((z_test.shape[0], n_bootstraps))
    for i in range(n_bootstraps):
        x_, z_ = resample(X_train, z_train)
        z_fit, zpred, beta = LinReg(x_, X_test, z_)
        z_pred[:, i] = zpred.ravel()
  
    polydegree[degree] = degree+1
    error[degree] = np.mean( np.mean((z_test - z_pred)**2, axis=1, keepdims=True) )
    bias[degree] = np.mean( (z_test - np.mean(z_pred, axis=1, keepdims=True))**2 )
    variance[degree] = np.mean( np.var(z_pred, axis=1, keepdims=True) )


plt.plot(polydegree, error, label='Error')
plt.plot(polydegree, bias, label='bias')
plt.plot(polydegree, variance, label='Variance')
plt.xticks(np.arange(1, maxdegree+1, step=1))  # Set label locations.
plt.xlabel('Model complexity: polynomial degree')
plt.ylabel('Mean squared error')
plt.title("Variance-Bias tradeoff for OLS")
plt.legend()
plt.savefig("Results/bias_variance_tradeoff/OLS_bias_var_tradeoff_v02.png",dpi=150)
plt.show()